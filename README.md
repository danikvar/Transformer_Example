# Transformer_Example
This is an assignment I did for a deep learning course implementing Transformer and RNN architecture from scratch to understand the difference in terms of a basic RNN

*   Section 1:

 Implemented a basic RNN cell, RNN Class, and an RNN Classifier
 
*   Section 2:

  Implemented a Transformer-based Text classifier and its components such as a Multi-head Attention Module, Positional Encoding Module, and Encoder

*   Section 3:

  To experiment with Decoders for a Transformer, implemented a Transformer-Based Machine Translation class using the modules from Section 2, a Decoder, Attention Masks, and a Seq-Seq Module
