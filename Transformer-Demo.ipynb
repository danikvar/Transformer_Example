{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Example\n",
    "\n",
    "This is an assignment I did for a deep learning course implementing Transformer architecture and understanding the difference in terms of a basic RNN\n",
    "\n",
    "*   Section 1:\n",
    "\n",
    " I implmented a basic RNN cell, RNN Class and an RNN Classifier\n",
    " \n",
    "*   Section 2:\n",
    "\n",
    "  Implemented a Transformer based Text classifier adn its components such as Multi-head Attention Module, Positional Encoding Module and Encoder\n",
    "\n",
    "*   Section 3:\n",
    "\n",
    "  In order to experiment with Decoders for a Transformer, I implemented a Transformer Based Machine Translation class using modules made in Section 2, a Decoder, Attention Masks and Seq-Seq Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjJqMmmUQq3S"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AHghnPFNQtqz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.6.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (1.10.13)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (0.10.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (22.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.6.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from jinja2->spacy==3.6.0) (2.1.1)\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 54.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (22.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n",
      "     --------------------------------------- 14.6/14.6 MB 65.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from de-core-news-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.10.13)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Requirement already satisfied: torchdata in c:\\users\\newdan\\anaconda3\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchdata) (1.26.14)\n",
      "Requirement already satisfied: torch>=2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchdata) (2.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchdata) (2.28.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch>=2->torchdata) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch>=2->torchdata) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch>=2->torchdata) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch>=2->torchdata) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch>=2->torchdata) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch>=2->torchdata) (1.11.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests->torchdata) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests->torchdata) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from jinja2->torch>=2->torchdata) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from sympy->torch>=2->torchdata) (1.2.1)\n",
      "Requirement already satisfied: torchtext in c:\\users\\newdan\\anaconda3\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchtext) (2.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchtext) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (1.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (4.4.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2022.11.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2.8.4)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests->torchtext) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1->torchtext) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1->torchtext) (1.2.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\newdan\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from seaborn) (3.7.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (22.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\newdan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy==3.6.0\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!pip install torchdata\n",
    "!pip install -U torchtext\n",
    "!pip install portalocker>=2.0.0\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln34q0UvNCJJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkqkCJ3s1pt5"
   },
   "source": [
    "# Section 1: Recurrent Neural Networks (RNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5YvJ8p54BVw"
   },
   "source": [
    "\n",
    "Each RNN Cell should contain 2 components: an Input Unit and a Hidden Unit. The Hidden state is the part of the RNN that remembers context about previous data present in the sequence. The current time step's hidden state is calculated using information of the previous time steps hidden state and the current input. This process helps to retain information on what the model saw in the previous time step when processing the current time steps information.\n",
    "\n",
    "RNNs will look and function as follows,\n",
    "\n",
    "<img src=\"images\\RNN Multi.png\">\n",
    "\n",
    "The hidden state any given time *t* is given by,\n",
    "\n",
    "\\begin{align}\n",
    "input_t &= (x_t \\cdot W_x^t + b_x^t) \\\\\n",
    "prev\\_state &= (h_{t-1} \\cdot W^t_h + b_h^t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "h_{t} &= tanh( input_t+ prev\\_state)\n",
    "\\end{align}\n",
    "\n",
    "The output at any give time *t* is given by,\n",
    "\n",
    "\\begin{align}\n",
    "y_t = h_t \\cdot W_y^t + b_y\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g67cCQBU3c_p"
   },
   "source": [
    "## 1.1 A Single RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcyGQaLpulEt"
   },
   "outputs": [],
   "source": [
    "class RNNCell(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RNNCell is a single cell that takes x_t and h_{t_1} as input and outputs h_t.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        \"\"\"\n",
    "        Constructor of RNNCell.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input x_t\n",
    "        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n",
    "        \"\"\"\n",
    "\n",
    "        # We always need to do this step to properly implement the constructor\n",
    "        super(RNNCell, self).__init__()\n",
    "\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Define the linear transformation layers and the non_linear layer. #\n",
    "        #    NonLinear will be Tanh here                                          #\n",
    "        ###########################################################################\n",
    "\n",
    "        self.linear_x = torch.nn.Linear(input_dim, hidden_dim, bias = True)\n",
    "        self.linear_h = nn.Linear(hidden_dim, hidden_dim, bias = True)\n",
    "        self.non_linear = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x_cur: torch.Tensor, h_prev: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute h_t given x_t and h_{t-1}.\n",
    "\n",
    "        Inputs:\n",
    "        - x_cur: x_t, a tensor with the same of BxC, where B is the batch size and\n",
    "          C is the channel dimension.\n",
    "        - h_prev: h_{t-1}, a tensor with the same of BxH, where H is the channel\n",
    "          dimension.\n",
    "        \"\"\"\n",
    "        h_cur = None\n",
    "        input = self.linear_x(x_cur)\n",
    "        prev_state = self.linear_h(h_prev)\n",
    "        h_cur = self.non_linear(input + prev_state)\n",
    "        return h_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnUpMZYP4tjd"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "x = torch.randn((2, 8)) # Input Dim\n",
    "h = torch.randn((2, 16)) # Hidden Dim\n",
    "\n",
    "model = RNNCell(8, 16)\n",
    "y = model(x , h)\n",
    "assert len(y.shape) == 2 and y.shape[0] == 2 and y.shape[1] == 16\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtKx0smPNlBO"
   },
   "source": [
    "## 1.2 RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rx2h1DoaNlRY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRNN\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    RNN is a single-layer (stack) RNN by connecting multiple RNNCell together in a single\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    direction, where the input sequence is processed from left to right.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim: \u001b[38;5;28mint\u001b[39m, hidden_dim: \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RNN is a single-layer (stack) RNN by connecting multiple RNNCell together in a single\n",
    "    direction, where the input sequence is processed from left to right.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        \"\"\"\n",
    "        Constructor of the RNN module.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input x_t\n",
    "        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_cell = RNNCell(input_dim, hidden_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the hidden representations for every token in the input sequence.\n",
    "\n",
    "        Input:\n",
    "        - x: A tensor with the shape of BxLxC, where B is the batch size, L is the squence\n",
    "          length, and C is the channel dimmension\n",
    "\n",
    "        Return:\n",
    "        - h: A tensor with the shape of BxLxH, where H is the hidden dimension of RNNCell\n",
    "        \"\"\"\n",
    "        b = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # initialize the hidden dimension\n",
    "        init_h = x.new_zeros((b, self.hidden_dim))\n",
    "        h = []\n",
    "        for i in range(seq_len):\n",
    "            next_h = self.rnn_cell(x[:, i, :], init_h)\n",
    "            h.append(next_h)\n",
    "            init_h = next_h\n",
    "\n",
    "       \n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2, 10, 8))\n",
    "#print(x.new_zeros(2, 10, 8))\n",
    "#print(x)\n",
    "\n",
    "\n",
    "b = x.shape[0]\n",
    "seq_len = x.shape[1]\n",
    "\n",
    "# initialize the hidden dimension\n",
    "init_h = x.new_zeros((b, 16))\n",
    "h = None\n",
    "print(init_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1699287078348,
     "user": {
      "displayName": "Aanand .D",
      "userId": "04055414698771147992"
     },
     "user_tz": 300
    },
    "id": "pTrPvUkEOXrR",
    "outputId": "f6d3c65b-be2f-49df-82a2-00f0911fee5b"
   },
   "outputs": [],
   "source": [
    "# Let's run a sanity check\n",
    "x = torch.randn((2, 10, 8))\n",
    "model = RNN(8, 16)\n",
    "y = model(x)\n",
    "assert len(y.shape) == 3\n",
    "for dim, dim_gt in zip(y.shape, [2, 10, 16]):\n",
    "    assert dim == dim_gt\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gySwzL67ObkF"
   },
   "source": [
    "## 1.3 RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gyqFZKgOaoR"
   },
   "outputs": [],
   "source": [
    "h_tracker = {}\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A RNN-based classifier for text classification. It first converts tokens into word embeddings.\n",
    "    And then feeds the embeddings into a RNN, where the hidden representations of all tokens are\n",
    "    then averaged to get a single embedding of the sentence. It will be used as input to a linear\n",
    "    classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            vocab_size: int, embed_dim: int, rnn_hidden_dim: int, num_class: int, pad_token: int\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Inputs:\n",
    "        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n",
    "        - embed_dim: The dimension of word embeddings\n",
    "        - rnn_hidden_dim: The hidden dimension of the RNN.\n",
    "        - num_class: Number of classes.\n",
    "        - pad_token: The index of the padding token.\n",
    "        \"\"\"\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        # word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
    "\n",
    "        self.rnn, self.fc = None, None\n",
    "\n",
    "        self.rnn, self.fc = RNN(embed_dim, rnn_hidden_dim), nn.Linear(rnn_hidden_dim, num_class, bias = True)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Get classification scores (logits) of the input.\n",
    "\n",
    "        Input:\n",
    "        - text: Tensor with the shape of BxLxC.\n",
    "\n",
    "        Return:\n",
    "        - logits: Tensor with the shape of BxK, where K is the number of classes\n",
    "        \"\"\"\n",
    "\n",
    "        # get word embeddings\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        logits = None\n",
    "        \n",
    "        rnn_out = self.rnn(embedded)\n",
    "        rnn_out = rnn_out.mean(dim=1)\n",
    "        logits = self.fc(rnn_out)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1699287078605,
     "user": {
      "displayName": "Aanand .D",
      "userId": "04055414698771147992"
     },
     "user_tz": 300
    },
    "id": "j3Bu8qExOwmO",
    "outputId": "b895fa43-33c5-4e4a-eff4-89b01e6aa80e"
   },
   "outputs": [],
   "source": [
    "# Sanity check!!!\n",
    "vocab_size = 10\n",
    "embed_dim = 16\n",
    "rnn_hidden_dim = 32\n",
    "num_class = 4\n",
    "\n",
    "x = torch.arange(vocab_size).view(1, -1)\n",
    "x = torch.cat((x, x), dim=0)\n",
    "print('x.shape: {}'.format(x.shape))\n",
    "model = RNNClassifier(vocab_size, embed_dim , rnn_hidden_dim, num_class, 0)\n",
    "y = model(x)\n",
    "assert len(y.shape) == 2 and y.shape[0] == 2 and y.shape[1] == num_class\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zZ-wowAQcmI"
   },
   "source": [
    "## Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'portalocker>=2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check here for details https://github.com/pytorch/text/blob/main/torchtext/data/utils.py#L52-#L166\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab_factory.py#L65-L113\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# Documentation of DataLoader https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15361,
     "status": "ok",
     "timestamp": 1699287093964,
     "user": {
      "displayName": "Aanand .D",
      "userId": "04055414698771147992"
     },
     "user_tz": 300
    },
    "id": "A7Uhb_tcQewa",
    "outputId": "f6e6035e-a8ae-441f-cdc7-4469587073cb"
   },
   "outputs": [],
   "source": [
    "# check here for details https://github.com/pytorch/text/blob/main/torchtext/data/utils.py#L52-#L166\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab_factory.py#L65-L113\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# Documentation of DataLoader https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# A tokenizer that splits a input setence into a set of tokens, including those puncuation\n",
    "\n",
    "# >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
    "# >>> tokens\n",
    "# >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Creates a vocab object which maps tokens to indices\n",
    "# Check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab.py\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "\n",
    "# The specified token will be returned when a out-of-vocabulary token is queried.\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "# The padding token we need to use\n",
    "# The returned indices are always in an array\n",
    "PAD_TOKEN = vocab(tokenizer('<pad>'))\n",
    "assert len(PAD_TOKEN) == 1\n",
    "PAD_TOKEN = PAD_TOKEN[0]\n",
    "\n",
    "\n",
    "# Merges a list of samples to form a mini-batch of Tensor(s)\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - batch: A list of data in a mini batch, where the length denotes the batch size.\n",
    "      The actual context depends on a particular dataset. In our case, each position\n",
    "      contains a label and a Tensor (tokens in a sentence).\n",
    "\n",
    "    Returns:\n",
    "    - batched_label: A Tensor with the shape of (B,)\n",
    "    - batched_text: A Tensor with the shape of (B, L, C), where L is the sequence length\n",
    "      and C is the channeld dimension\n",
    "    \"\"\"\n",
    "    label_list, text_list, text_len_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        text_len_list.append(processed_text.size(0))\n",
    "    batched_label, batched_text = None, None\n",
    "\n",
    "    max_len = max(text_len_list)\n",
    "    # pad all values to same length and create a single B X MAX_LENGTH X C TENSOR\n",
    "    batched_text = torch.nn.utils.rnn.pad_sequence(text_list,\n",
    "                                                   batch_first=True,\n",
    "                                                   padding_value  = PAD_TOKEN)\n",
    "    # labels into tensor\n",
    "    batched_label = torch.tensor(label_list)\n",
    "\n",
    "\n",
    "    return batched_label.long(), batched_text.long()\n",
    "\n",
    "# Now, let's check what the batched data looks like\n",
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "for idx, (label, data) in enumerate(dataloader):\n",
    "    if idx > 0:\n",
    "        break\n",
    "    print('label.shape: {}'.format(label.shape))\n",
    "    print('label: {}'.format(label))\n",
    "    print('data.shape: {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A look at our processed text labels\n",
    "label_list, text_list, text_len_list = [], [], []\n",
    "cur_num = 0\n",
    "for (_label, _text) in train_iter:\n",
    "    label_list.append(label_pipeline(_label))\n",
    "    processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "    text_list.append(processed_text)\n",
    "    text_len_list.append(processed_text.size(0))\n",
    "    if cur_num < 10:\n",
    "        print(_label, _text)\n",
    "        print(processed_text)\n",
    "        print(max(text_len_list))\n",
    "        print(\"******************************************************************\")\n",
    "        print(\"******************************************************************\")\n",
    "        print(\"******************************************************************\")\n",
    "        print(\"\\n\\n\")\n",
    "    cur_num += 1\n",
    "\n",
    "batched_label, batched_text = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4782,
     "status": "ok",
     "timestamp": 1699287098736,
     "user": {
      "displayName": "Aanand .D",
      "userId": "04055414698771147992"
     },
     "user_tz": 300
    },
    "id": "O8gaIhlS-KPz",
    "outputId": "d092d9a5-ccff-4089-aef1-dfba9ce69f18"
   },
   "outputs": [],
   "source": [
    "labels=set()\n",
    "labels.update([entry[0] for entry in AG_NEWS(root=\"data\")[0]])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k90mvB7GPk2x"
   },
   "source": [
    "## 1.4 Train & Evaluate Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBeUMCGnPkEx"
   },
   "outputs": [],
   "source": [
    "\n",
    "# logits_tracker = {}\n",
    "def train(model, dataloader, loss_func, device, grad_norm_clip, optimizer):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    global logits_tracker\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        label = label.to(device)\n",
    "        text = text.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(text)\n",
    "        loss = loss_func(logits, label)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "        optimizer.step()\n",
    "        total_acc += (logits.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, dataloader, loss_func, device):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            logits = model(text)\n",
    "            loss = loss_func(logits, label)\n",
    "\n",
    "            total_acc += (logits.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrskpzBbRnu6"
   },
   "outputs": [],
   "source": [
    "\n",
    "assert torch.cuda.is_available(), \"Please connect to the GPU instance if working on Colab or configure the environment for Torch using GPU (Comment this line if not using GPU)\"\n",
    "# device = 'cuda'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyper parameters\n",
    "epochs = 3 # epoch\n",
    "lr =0.0005 # learning rate\n",
    "batch_size = 64 # batch size for training\n",
    "word_embed_dim = 64\n",
    "rnn_hidden_dim = 96\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model, loss_func = None, None\n",
    "\n",
    "\n",
    "\n",
    "model = RNNClassifier(vocab_size, word_embed_dim,\n",
    "                      rnn_hidden_dim, num_class, \n",
    "                      pad_token = PAD_TOKEN)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# copy the model to the specified device (GPU)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1699287109588,
     "user": {
      "displayName": "Aanand .D",
      "userId": "04055414698771147992"
     },
     "user_tz": 300
    },
    "id": "Jw0UjFJ6SXIH",
    "outputId": "01088211-3a0e-4d39-93f7-a1ca33e62718"
   },
   "outputs": [],
   "source": [
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset,\n",
    "    [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=batch_size,\n",
    "    shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=batch_size,\n",
    "    shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size,\n",
    "    shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "split_train_[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 294881,
     "status": "ok",
     "timestamp": 1699287404459,
     "user": {
      "displayName": "Aanand .D",
      "userId": "04055414698771147992"
     },
     "user_tz": 300
    },
    "id": "_bR_1gj7SXvN",
    "outputId": "4d1eb428-c9bc-46cf-8fee-5a41dd9dad97"
   },
   "outputs": [],
   "source": [
    "# You should be able get a validation accuracy around 86%\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # global logits_tracker\n",
    "    # logits_tracker[epoch] = None\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_dataloader, loss_func, device, 1, optimizer)\n",
    "    accu_val = evaluate(model, valid_dataloader, loss_func, device)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n",
    "    \n",
    "#-----------------------------------------------------------\n",
    "#| end of epoch   3 | time: 82.78s | valid accuracy    0.881 \n",
    "#-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2Ca9byg078p"
   },
   "source": [
    "# Section 2: Transformers - 'Attention is All you Need' : Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are a type of deep learning architecture that has had a profound impact on a wide range of natural language processing (NLP) tasks and other sequence-to-sequence tasks. They are known for their ability to model long-range dependencies and their parallelization capabilities. A typical transformer model consists of several key components:\n",
    "\n",
    "<img src=\"images\\The-Transformer-model-architecture.png\">\n",
    "\n",
    "Transformers have revolutionized NLP and have been adapted for a wide range of applications beyond text, including image generation, recommendation systems, and more. For this section, we will be implementing, all but two important components, Attention-Masks and Decoder Module. They will be implemented in-depth in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ldgv5vkemPN6"
   },
   "source": [
    "## 2.1 Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\MHA.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmKX9MmKpTAl"
   },
   "source": [
    "Multi-Head Attention can be mathematically explained as follows:\n",
    "\n",
    "Let's assume we have a sequence of input vectors $(X = {x_1, x_2, ...., x_n})$, where ($x_i$) represents the ($i$)-th element of the sequence. Each $x_i$ is typically a vector, such as a word embedding in natural language processing.\n",
    "\n",
    "\n",
    "\n",
    "1. **Single Attention Head:**\n",
    "   - In a single attention head, we compute attention scores ($A_{ij}$) between every pair of input elements ($x_i$) and ($x_j$). These scores are computed using a compatibility function, often a dot product or a learned linear transformation followed by a softmax activation:\n",
    "\n",
    "   $\\begin{align}\n",
    "   A_{ij} = {softmax}({(Q_ix_i)^T(K_jx_j)}{\\sqrt{d_k}})\n",
    "  \\end{align}\n",
    "   $\n",
    "   Where $Q_i$ and $K_j$ are learned linear transformations of the input vectors $x_i$ and $x_j$, and $d_k$ is the dimension of the key vectors.\n",
    "\n",
    "   - The attention scores are used to compute weighted representations of the input sequence:\n",
    "\n",
    "   \\begin{align}\n",
    "   \\text{Attention}(X) = \\sum_{j=1}^{n} A_{ij}V_j\n",
    "   \\end{align}\n",
    "\n",
    "   Where $V_j$ is a learned linear transformation of the input vector $x_j$.\n",
    "\n",
    "2. **Multiple Attention Heads:**\n",
    "   - In Multi-Head Attention, we use $H$ attention heads in parallel. Each head has its own sets of learned parameters for $Q$, $K$, and $V$, resulting in $H$ sets of attention scores and weighted representations.\n",
    "   \n",
    "    \\begin{align}\n",
    "   MultiHead(X) = Concatenate(Head_1, Head_2,..., Head_H).W^O\n",
    "   \\end{align}\n",
    "\n",
    "   Where $W^O$ is another learned linear transformation applied to the concatenated outputs, and $Head_i$ represents the output of the $i$-th attention head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBnKpnQ5lplW"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that computes multi-head attention given query, key, and value tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Dimension of the input query, key, and value. Here we assume they all have\n",
    "          the same dimensions. But they could have different dimensions in other problems.\n",
    "        - num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert input_dim % num_heads == 0 # Check if we can get back the original Dimensions!\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_per_head = input_dim // num_heads\n",
    "\n",
    "        # Technically have H different weight matrices for k,v,q\n",
    "          # stack them -> input_dim = num_heads*dim_per_head\n",
    "        self.k = nn.Linear(input_dim, input_dim, bias = True)\n",
    "        self.v = nn.Linear(input_dim, input_dim, bias = True)\n",
    "        self.q = nn.Linear(input_dim, input_dim, bias = True)\n",
    "        self.output = nn.Linear(input_dim, input_dim, bias = True)\n",
    "        \n",
    "\n",
    "        self.scores = None\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Compute the attended feature representations.\n",
    "\n",
    "        Inputs:\n",
    "        - query: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - key: Tensor of the shape BxLxC\n",
    "        - value: Tensor of the shape BxLxC\n",
    "        - mask: Tensor indicating where the attention should *not* be performed\n",
    "        \"\"\"\n",
    "        b = query.shape[0]\n",
    "\n",
    "        dot_prod_scores = None\n",
    "\n",
    "        k_head_shape = key.shape[:-1]\n",
    "\n",
    "        dk_root = torch.sqrt(torch.tensor(self.dim_per_head))\n",
    "\n",
    "        query = self.q(query)\n",
    "        key = self.k(key)\n",
    "        value = self.v(value)\n",
    "\n",
    "        #print(query.shape)\n",
    "        query = query.reshape(*q_head_shape, self.num_heads, self.dim_per_head)\n",
    "        key = key.reshape(*k_head_shape, self.num_heads, self.dim_per_head)\n",
    "        value = value.reshape(*k_head_shape, self.num_heads, self.dim_per_head)\n",
    "\n",
    "        # WANT BATCH X HEADS X LENGTH X DIM\n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "\n",
    "\n",
    "        # a_ij = softmax( [Q_i*x_i]^T * [K_i*x_i] *sqrt(d_k))\n",
    "        # Attention(X) = SUM(A_ij * V_j)\n",
    "        \"\"\"\n",
    "        x = x.reshape(*head_shape, num_heads, dim_per_head)\n",
    "        x2 = x2.reshape(*head_shape, num_heads, dim_per_head)\n",
    "        x = x.transpose(1,2)\n",
    "        x2 = x2.transpose(1,2)\n",
    "        print(x.shape)\n",
    "        print(x2.shape)\n",
    "        #print(x2.shape)\n",
    "        # WANT BATCH X HEADS X LENGTH X DIM\n",
    "        # SO 2 X 4 X 10 X 2\n",
    "        print(torch.matmul(x, x2.transpose(-2, -1)).shape)\n",
    "        print(torch.einsum('bhid,bhjd->bhij', x, x2).shape)\n",
    "        # INPUT_DIM X BATCH X INPUT_DIM (EMBEDDING DIM)\n",
    "        \"\"\"\n",
    "\n",
    "        # multiply over batch, heads, query_tokens, key_tokens\n",
    "        # sum over d_k\n",
    "        # output is batch x heads x [query x key summed over d_k]\n",
    "        dot_prod_scores = torch.einsum('bhid,bhjd->bhij', query, key) / dk_root\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            # We simply set the similarity scores to be near zero for the positions\n",
    "            # where the attention should not be done\n",
    "            dot_prod_scores = dot_prod_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        out = None\n",
    "        ###########################################################################\n",
    "        # Compute the attention scores, which are then used to modulate the       #\n",
    "        # value tensor. Finally concate the attended tensors from multiple heads  #\n",
    "        # and feed it into the output layer.\n",
    "        ###########################################################################\n",
    "        attn = torch.nn.functional.softmax(dot_prod_scores, dim = 1)\n",
    "        attn = torch.einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "        # headshape = (batch, seq_len)\n",
    "\n",
    "        #print(attn.shape)\n",
    "        cat_attn = attn.reshape(*q_head_shape, -1)\n",
    "        out = self.output(cat_attn)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNS1Elc9wvhc"
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "x = torch.randn((2, 10, 8))\n",
    "mask = torch.randn((2, 10)) > 0.5\n",
    "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
    "num_heads = 4\n",
    "model = MultiHeadAttention(8, num_heads)\n",
    "y = model(x, x, x, mask)\n",
    "assert len(y.shape) == len(x.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Dimensions for batches\n",
    "\n",
    "\n",
    "x = torch.randn((2, 10, 8))\n",
    "x2 = torch.randn((2, 10, 8))\n",
    "x_val = torch.randn((2, 10, 8))\n",
    "# BATCH X LENGTH X DIM_MODEL\n",
    "num_heads = 4\n",
    "input_dim = 8\n",
    "dim_per_head = input_dim // num_heads\n",
    "#2\n",
    "\n",
    "head_shape = x.shape[:-1]\n",
    "\n",
    "#print(head_shape)\n",
    "x = nn.Linear(input_dim, input_dim)(x)\n",
    "x2 = nn.Linear(input_dim, input_dim)(x2)\n",
    "x_val = nn.Linear(input_dim, input_dim)(x_val)\n",
    "#print(x.shape)\n",
    "\n",
    "head_shape = x.shape[:-1]\n",
    "#print(torch.matmul(x, x2.transpose(-2, -1)).shape)\n",
    "\n",
    "\n",
    "x = x.reshape(*head_shape, num_heads, dim_per_head)\n",
    "\n",
    "\n",
    "x2 = x2.reshape(*head_shape, num_heads, dim_per_head)\n",
    "x_val = x_val.reshape(*head_shape, num_heads, dim_per_head)\n",
    "\n",
    "\n",
    "x = x.transpose(1,2)\n",
    "x2 = x2.transpose(1,2)\n",
    "x_val = x_val.transpose(1,2)\n",
    "\n",
    "print(x.shape)\n",
    "print(x2.shape)\n",
    "\n",
    "#transpose the last 2 dims\n",
    "print(x2.transpose(-2, -1).shape)\n",
    "\n",
    "# WANT BATCH X HEADS X LENGTH X DIM\n",
    "# SO 2 X 4 X 10 X 2\n",
    "print((torch.matmul(x, x2.transpose(-2, -1))/torch.sqrt(torch.tensor(dim_per_head))).shape)\n",
    "\n",
    "print((torch.matmul(x, x2.transpose(-2, -1))/torch.sqrt(torch.tensor(dim_per_head)))[0,0,5,1])\n",
    "# ID X DJ = IXJ OUTPUT MATRIX --> I = input_length\n",
    "x3 = torch.einsum('bhid,bhjd->bhij', x, x2)/torch.sqrt(torch.tensor(dim_per_head))\n",
    "print(x3.shape)\n",
    "print(x3[0,0,5,1])\n",
    "\n",
    "# INPUT_DIM X\n",
    "print(torch.matmul(x3, x_val)[0,0,1,1])\n",
    "print(torch.einsum(\"bhij,bhjd->bhid\", x3, x_val)[0,0,1,1])\n",
    "\n",
    "finx = torch.einsum(\"bhij,bhjd->bhid\", x3, x_val)\n",
    "finx = finx.reshape(*head_shape, -1)\n",
    "print(finx.shape)\n",
    "#print(x3)\n",
    "# INPUT_DIM X BATCH X INPUT_DIM (EMBEDDING DIM)\n",
    "#print(torch.matmul(x3, x).shape)\n",
    "#x2.transpose(-2, -1).shape\n",
    "\n",
    "# value i * value j at batch b, head h, and dim d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuRsu7Usw8hC"
   },
   "source": [
    "## 2.2 Positional Encoding Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq0T_lmV-hT3"
   },
   "source": [
    "Positional Encoding is a critical component in the Transformer architecture, designed to provide information about the positions of elements in a sequence to a model that inherently lacks sequential information. Transformers use self-attention mechanisms that do not inherently understand the order or position of tokens in the input. Positional Encoding is introduced to address this limitation and allow the model to consider the order of elements within the input sequence.\n",
    "\n",
    "It addresses the challenge of modeling sequences with self-attention mechanisms that do not inherently understand the order of elements. By adding Positional Encoding to the input embeddings, the model can differentiate between tokens based on their positions and capture sequential information effectively.\n",
    "\n",
    "\n",
    "1. **Positional Encoding Function:**\n",
    "   - Positional Encoding is typically represented as a fixed-size vector that is added element-wise to the input embeddings. This vector is determined by a mathematical function.\n",
    "   - The most common approach is to use a combination of sine and cosine functions with different frequencies and phases to create a unique encoding for each position.\n",
    "   - For each position $pos$ and dimension $i$ of the Positional Encoding vector, $PE(pos, 2i)$ is given by\n",
    "   \n",
    "  \\begin{align}\n",
    "  \\sin\\left(\\frac{pos}{10000^{(2i/d_{\\text{model}})}}\\right) & \\text{if } i \\text{ is even} \\\\\n",
    "  \\cos\\left(\\frac{pos}{10000^{(2i/d_{\\text{model}})}}\\right) & \\text{if } i \\text{ is odd}\n",
    "  \\end{align}\n",
    "\n",
    " *$d_{model}$ is the dimension of the model's input embeddings.*\n",
    "\n",
    "3. **Adding Positional Encoding:**\n",
    "   - The Positional Encoding vector is added element-wise to the input embeddings. This combination of the original word embeddings and the Positional Encoding allows the model to distinguish between tokens based on their positions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3tNlP38HbML"
   },
   "source": [
    "### 2.2.1 Let's Try to work an example!\n",
    "Assume the sentence coming into the encoding is, \"This is an Example\"; The Positional Encoding layer is initialized with the following parameters;\n",
    "\n",
    "* $k$ = $0\\leq k < L $ =$\\boxed{4}$\n",
    "Should this be the 10,000 max pos??\n",
    "* $n = 100$\n",
    "* $d = 6$\n",
    "* $I$ = $0 \\leq i < d/2$ = $\\boxed{3}$\n",
    "*  Based on the values above, for the text given, Find the Position Encoding values below or create on on your own and upload to this section! ***DONT CODE IT***\n",
    "\n",
    "|          |   |           |   | Positional Encodings d = 6 & n = 100 |     |     |     |     |     |     |     |\n",
    "|----------|---|-----------|---|:------------------------------------:|-----|-----|-----|-----|-----|-----|-----|\n",
    "| Sequence |   | Index (k) |   | i=0                                  | i=0 | i=1 | i=1 | i=2 | i=2 | i=3 | i=3 |\n",
    "| This     |   | 0         |   | $P_{00} = 0$   |$P_{00} = 1$| $P_{01} = 0$|$P_{01} = 1$|$P_{02} = 0$|$P_{02} = 1$|$P_{03} = 0$|$P_{03} = 1$|\n",
    "| is       |   | 1         |   | $P_{10} =0.841 $|$P_{10} =  0.540$|$P_{11} = 0.046$|$P_{11} = 0.999$|$P_{12} = 0.002$|$P_{12} = 1.000$|$P_{13} = 0.000$|$P_{13} = 1.000$|\n",
    "| an       |   | 2         |   | $P_{20} = 0.909$|$P_{20} = -0.416$|  $P_{21} = 0.093$|$P_{21} = 0.996$|$P_{22} = 0.004$|$P_{22} = 1.000$|$P_{23} = 0.000$|$P_{23} = 1.000$|            \n",
    "| Example  |   | 3         |   | $P_{30} = 0.141$|$P_{30} = -0.990$| $P_{31} = 0.139$|$P_{31} = 0.990$|$P_{32} = 0.006$|$P_{32} =1.000$|$P_{33} = 0.000$|$P_{33} = 1.000$|\n",
    "\n",
    "* There should be no i = 3? Since d/2 = 3 and i < 3?\n",
    "* I checked to make sure that all of these values are unique however adding more than 3 decimal places would make this table large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Td0eavTHgs1"
   },
   "source": [
    "### 2.2.1 Postional Encoding Code\n",
    "\n",
    "Implementation of the Positional Encoding part.\n",
    "\n",
    "* torch.arange() : https://pytorch.org/docs/stable/generated/torch.arange.html\n",
    "* torch.stack() : https://pytorch.org/docs/stable/generated/torch.stack.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zhA3ED4w_0H"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that adds positional encoding to each of the token's features.\n",
    "    So that the Transformer is position aware.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, max_len: int=10000):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension about the features for each token\n",
    "        - max_len: The maximum sequence length\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the positional encoding and add it to x.\n",
    "\n",
    "        Input:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "\n",
    "        Return:\n",
    "        - x: Tensor of the shape BxLxC, with the positional encoding added to the input\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        input_dim = x.shape[2]\n",
    "\n",
    "        pe = None\n",
    "    \n",
    "        # Row for every position, k, and a column for each 2i (i sin, i cos)\n",
    "        encodings = torch.zeros(seq_len, input_dim)\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        #Go from 0-D counting by two --> same as (0 - D/2) * 2\n",
    "        two_i = torch.arange(0, input_dim, 2, dtype=torch.float32)\n",
    "\n",
    "        #print(encodings.shape)\n",
    "        #print(encodings)\n",
    "\n",
    "        #print(position.shape)\n",
    "        #print(position)\n",
    "\n",
    "        #print(two_i.shape)\n",
    "        #print(two_i)\n",
    "\n",
    "        #div_term1 = torch.exp(two_i * -(math.log(self.max_len) / input_dim))\n",
    "        # equivalentish\n",
    "        div_term = 1/torch.pow(self.max_len,(two_i/input_dim))\n",
    "        # from 0,1 by 2 in cols, every row\n",
    "        encodings[:, 0::2] = torch.sin(position * div_term)\n",
    "        encodings[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add batch size since x is BxLxC\n",
    "        pe = encodings\n",
    "        #.requires_grad_(False)\n",
    "\n",
    "\n",
    "        x = x + pe.to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1AbsDpAxEmi"
   },
   "outputs": [],
   "source": [
    "# Sanity check - I\n",
    "x = torch.randn(1, 100, 20)\n",
    "pe = PositionalEncoding(20)\n",
    "y = pe(x)\n",
    "assert len(x.shape) == len(y.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glfitwCKUc2E"
   },
   "outputs": [],
   "source": [
    "# Sanity Check - II\n",
    "x = torch.randn(1, 100, 6)\n",
    "d = 6\n",
    "n = 100\n",
    "pe = PositionalEncoding(d,n)\n",
    "y = pe(x)\n",
    "\n",
    "y -= x\n",
    "print(y[:,:4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1CfurktZuKi"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1699287405200,
     "user": {
      "displayName": "Aanand .D",
      "userId": "04055414698771147992"
     },
     "user_tz": 300
    },
    "id": "15RK-geDxGOI",
    "outputId": "9b832038-1a85-40e6-f27d-045b1163115d"
   },
   "outputs": [],
   "source": [
    "# Sanity check - III\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20)\n",
    "y = pe.forward((torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSJX3Bnk3Wif"
   },
   "source": [
    "<img src=\"images\\PE Wave.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWXhGiwFAVI_"
   },
   "source": [
    "## 2.3 FeedForward Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNloj6m5CpxJ"
   },
   "source": [
    "The FeedForward Layer in the Transformer architecture is a position-wise neural network layer designed to process the context-aware representations generated by the self-attention mechanism. It consists of two linear transformations followed by a non-linear activation function, typically ReLU.  The FeedForward Layer is applied independently to each position in the sequence, allowing the model to capture different patterns at different positions. This position-wise independence, combined with non-linearity, helps the model learn complex relationships within the data and plays a crucial role in the Transformer's ability to process and understand sequential data effectively, making it a fundamental component for various sequence-to-sequence tasks.\n",
    "\n",
    "Mathematically, if $X$ represents the input sequence (a sequence of embeddings), $FFN(X)$ is the output of the FeedForward Layer, and $W_1$, $W_2$, $b_1$, and $b_2$ represent learned weight matrices and bias terms, the operation can be expressed as,\n",
    "\n",
    "\\begin{align}\n",
    "FFN(X) = ReLU(X.W_i + b_i) . W_2 + b_2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnhMfzr_AXDO"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward network. Essentially, it is a two-layer fully-connected\n",
    "    neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, ff_dim, dropout):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension\n",
    "        - ff_dim: Hidden dimension\n",
    "        \"\"\"\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        \n",
    "        # Define the two linear layers and a non-linear one.\n",
    "        \n",
    "        # no need for dropout in FF layer\n",
    "        self.linear1 = nn.Linear(input_dim, ff_dim, bias = True)\n",
    "        self.linear2 = nn.Linear(ff_dim, input_dim, bias = True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "         and C is the channel dimension\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of the shape BxLxC\n",
    "        \"\"\"\n",
    "\n",
    "        y = None\n",
    "        \n",
    "        # Process the input.                                               \n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        y = self.linear2(x)\n",
    "        \n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu1ZP598AZOw"
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "x = torch.randn((2, 10, 8))\n",
    "ff_dim = 4\n",
    "model = FeedForwardNetwork(8, ff_dim, 0.1)\n",
    "y = model(x)\n",
    "assert len(x.shape) == len(y.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuQTU5FYAeAw"
   },
   "source": [
    "## 2.4 Encoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yim4YF3JWWPA"
   },
   "source": [
    "The Encoder module in a Transformer is responsible for processing the input sequence, typically used for tasks like language understanding and representation learning. It consists of multiple identical layers, each containing two main components: the Multi-Head Self-Attention mechanism and the Position-wise FeedForward Layer.\n",
    "\n",
    "<img src=\"images\\Encoder-module.png\" width=\"300\">\n",
    "\n",
    "*   In each layer, the input sequence is first passed through the Multi-Head Self-Attention mechanism, which computes weighted representations for each element in the sequence, capturing contextual information.The attention output is then passed through the Position-wise FeedForward Layer, introducing non-linearity and allowing the model to capture different patterns at each position.\n",
    "*   This process is repeated for each layer in the encoder stack, enabling the model to capture hierarchical features and   within the input sequence effectively. The final encoder output represents a rich contextualized representation of the input sequence, which can be used for various downstream tasks, including translation, text generation, and sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdP_tuKQHz09"
   },
   "source": [
    "### 2.4.1 Encoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h7bo0xlAgEo"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) for the Transformer encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderCell, self).__init__()\n",
    "\n",
    "        ###########################################################################\n",
    "        # A single Transformer encoder cell consists of\n",
    "        # 1. A multi-head attention module\n",
    "        # 2. Followed by dropout\n",
    "        # 3. Followed by layer norm (check nn.LayerNorm)\n",
    "        \n",
    "        # At the same time, it also has\n",
    "        # 1. A feedforward network\n",
    "        # 2. Followed by dropout\n",
    "        # 3. Followed by layer norm\n",
    "        ###########################################################################\n",
    "        self.multi_head_attention = MultiHeadAttention(input_dim, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(input_dim, ff_dim, dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.attention = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        y = None\n",
    "       \n",
    "        # Get the output of the multi-head attention part (with dropout     \n",
    "        # and layer norm), which is used as input to the feedforward network \n",
    "        \n",
    "        atn = self.multi_head_attention(x, x, x, mask)\n",
    "        x = x + self.dropout1(atn)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "\n",
    "        #if self.attention is None:\n",
    "        #  self.attention = [x.detach()]\n",
    "        #else:\n",
    "        #  self.attention.append(x.detach())\n",
    "\n",
    "        self.ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout2(self.ffn_output)\n",
    "        y = self.layer_norm2(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MofgAqe_Ajzp"
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "\n",
    "x = torch.randn((2, 10, 8))\n",
    "mask = torch.randn((2, 10)) > 0.5\n",
    "mask = mask.unsqueeze(1).unsqueeze(-1)\n",
    "num_heads = 4\n",
    "model = TransformerEncoderCell(8, num_heads, 32, 0.1)\n",
    "y = model(x, mask)\n",
    "assert len(x.shape) == len(y.shape)\n",
    "for dim_x, dim_y in zip(x.shape, y.shape):\n",
    "    assert dim_x == dim_y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAPO0P3oHWZo"
   },
   "source": [
    "### 2.4.2 Building an Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOc28jQpL9kd"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A full encoder consisting of a set of TransformerEncoderCell.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: Number of TransformerEncoderCells\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.norm = None # LayerNorm layer\n",
    "        self.cells = None # TransformerEncoderCells\n",
    "\n",
    "\n",
    "        # Store a stack of TranformerEncoderCell and define a layer normalization \n",
    "        # layer to process the output of the entire encoder.    \n",
    "        \n",
    "        self.cells = nn.ModuleList([TransformerEncoderCell(input_dim, num_heads, ff_dim, dropout) for _ in range(num_cells)])\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "        - mask: Tensor for multi-head attention\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of the shape of BxLxC, which is the normalized output of the encoder\n",
    "        \"\"\"\n",
    "\n",
    "        y = None\n",
    "\n",
    "        #Feed x into the stack of TransformerEncoderCells and then normalize the output with layer norm.                                   #\n",
    "\n",
    "        for cell in self.cells:\n",
    "            x = cell(x, mask)\n",
    "        y = self.norm(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRToOfz-U7NX"
   },
   "source": [
    "## 2.5 Transformer Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75PSJtbSbSlV"
   },
   "source": [
    "Now, lets put this all the above describled modules together to make out classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNGH9CHaU9Pv"
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer-based text classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            vocab_size: int, embed_dim: int, num_heads: int, trx_ff_dim: int,\n",
    "            num_trx_cells: int, num_class: int, dropout: float=0.1, pad_token: int=0\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n",
    "        - embed_dim: The dimension of word embeddings\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - trx_ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_trx_cells: Number of TransformerEncoderCells\n",
    "        - dropout: Dropout ratio\n",
    "        - pad_token: The index of the padding token.\n",
    "        \"\"\"\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # word embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
    "\n",
    "\n",
    "        # Define a module for positional encoding, Transformer encoder, and a output layer                                                          #\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        self.encoder = TransformerEncoder(embed_dim, num_heads, trx_ff_dim, num_trx_cells, dropout)\n",
    "        self.output = nn.Linear(embed_dim, num_class)\n",
    "       \n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - text: Tensor with the shape of BxLxC.\n",
    "        - mask: Tensor for multi-head attention\n",
    "\n",
    "        Return:\n",
    "        - logits: Tensor with the shape of BxK, where K is the number of classes\n",
    "        \"\"\"\n",
    "\n",
    "        # word embeddings, note we multiple the embeddings by a factor\n",
    "        embedded = self.embedding(text) * math.sqrt(self.embed_dim)\n",
    "\n",
    "        logits = None\n",
    "        \n",
    "        # Apply positional embedding to the input, which is then fed into the encoder. \n",
    "        # Average pooling is applied then to all the features of all tokens. \n",
    "        # Finally, the logits are computed based on the pooled features.\n",
    "        pe = self.pos_encoder(embedded)\n",
    "        x = self.encoder(pe, mask)\n",
    "        #print(\"shape1\")\n",
    "        #print(x.shape)\n",
    "        # Pool on the L dimension\n",
    "        x = torch.mean(x, dim=1)\n",
    "        #print(\"shape2\")\n",
    "        #print(x.shape)\n",
    "        logits = self.output(x)\n",
    "\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "810eXvOtU_KA"
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "vocab_size = 10\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "trx_ff_dim = 16\n",
    "num_trx_cells = 2\n",
    "num_class = 3\n",
    "\n",
    "x = torch.arange(vocab_size).view(1, -1)\n",
    "x = torch.cat((x, x), dim=0)\n",
    "mask = (x != 0).unsqueeze(-2).unsqueeze(1)\n",
    "model = TransformerClassifier(vocab_size, embed_dim, num_heads, trx_ff_dim, num_trx_cells, num_class)\n",
    "print('x: {}, mask: {}'.format(x.shape, mask.shape))\n",
    "y = model(x, mask)\n",
    "assert len(y.shape) == 2 and y.shape[0] == x.shape[0] and y.shape[1] == num_class\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6S703BMVr4S"
   },
   "source": [
    "## 2.6 Deciding HyperParameters & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the previous training code to remember\n",
    "# logits_tracker = {}\n",
    "def train(model, dataloader, loss_func, device, grad_norm_clip, optimizer):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    global logits_tracker\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        label = label.to(device)\n",
    "        text = text.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(text)\n",
    "        loss = loss_func(logits, label)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "        optimizer.step()\n",
    "        total_acc += (logits.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, dataloader, loss_func, device):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            logits = model(text)\n",
    "            loss = loss_func(logits, label)\n",
    "\n",
    "            total_acc += (logits.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oT2edMKVbHK"
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 3 # epoch\n",
    "lr = 0.0005  # learning rate\n",
    "batch_size = 64 # batch size for training\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "\n",
    "num_heads = 4\n",
    "num_trx_cells = 2\n",
    "\n",
    "gradient_norm_clip = 1\n",
    "\n",
    "# Transformer-based text classifier and a loss function.      \n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "model = TransformerClassifier(vocab_size, emsize, num_heads, trx_ff_dim, num_trx_cells, num_class)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
    "total_accu = None\n",
    "\n",
    "# Should be able to get a validation accuracy around 89%\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_dataloader, loss_func, device, gradient_norm_clip, optimizer)\n",
    "    accu_val = evaluate(model, valid_dataloader, loss_func, device)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n",
    "    mem_alloc = torch.cuda.memory_allocated()\n",
    "    mem_max = torch.cuda.max_memory_allocated()\n",
    "    print(\"Memory %:\")\n",
    "    print(mem_alloc/mem_max * 100)\n",
    "    print('-' * 59)\n",
    "    \n",
    "    \n",
    "\n",
    "#-----------------------------------------------------------\n",
    "#| end of epoch   3 | time: 28.67s | valid accuracy    0.887 \n",
    "#-----------------------------------------------------------\n",
    "#Memory %:\n",
    "#32.84459587512779\n",
    "#-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UC00oQqdR8_"
   },
   "source": [
    "# Section 3: Transformers - 'Attention is All you need' : Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\Decoder-module.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM6xGB79utJL"
   },
   "source": [
    "The decoder module in a Seq-Seq Transformer model is responsible for generating the output sequence based on the information gathered by the encoder and previous tokens in an autoregressive manner. It utilizes self-attention mechanisms with masking to enforce causality and multi-head attention to capture dependencies between tokens in the output. Self-attention calculates attention weights for each position in the sequence, and multi-head attention aggregates the results from multiple attention heads, enhancing the model's representational power. Cross-attention is also employed to allow the decoder to focus on relevant parts of the encoder's output.\n",
    "\n",
    "Additionally, position-wise feed-forward networks further process the information by applying linear transformations and non-linear activation functions to each position independently. Throughout the decoder, layer normalization and residual connections are utilized to enhance training stability. These components are typically stacked in multiple layers to enable the model to learn complex relationships and generate coherent output sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t22d9w_d4ia"
   },
   "source": [
    "### 3.1.1 Decoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQyVu87RdZfa"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) of the Transformer decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderCell, self).__init__()\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Similar to the TransformerEncoderCell, define two                 #\n",
    "        # MultiHeadAttention modules. One for processing the tokens on the        #\n",
    "        # decoder side. The other for getting the attention across the encoder.   #\n",
    "        # and the decoder. Also define a feedforward network. Don't forget the    #\n",
    "        # Dropout and Layer Norm layers.                                          #\n",
    "        ###########################################################################\n",
    "\n",
    "        self.sattn = MultiHeadAttention(input_dim, num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.cattn = MultiHeadAttention(input_dim, num_heads)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(input_dim, ff_dim, dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.layer_norm3 = nn.LayerNorm(input_dim)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
    "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
    "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
    "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
    "        \"\"\"\n",
    "\n",
    "        y = None\n",
    "        \n",
    "        # Compute the self-attended features for the tokens on the decoder side. \n",
    "        # Then compute the cross-attended features for the tokens on the decoder side \n",
    "        # to the encoded features, --> which are finally feed into the feedforward network                                                     #\n",
    "        \n",
    "        \n",
    "        #print(\"Check 1\")\n",
    "        self_atn = self.sattn(x, x, x, tgt_mask)\n",
    "        x = x + self.dropout1(self_atn)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        #print(\"Check 2\")\n",
    "        #print(x.shape)\n",
    "        #print(encoder_output.shape)\n",
    "        cross_atn = self.cattn(x, encoder_output, encoder_output, src_mask)\n",
    "        x = x + self.dropout2(cross_atn)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        #print(\"Check 3\")\n",
    "        self.ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout3(self.ffn_output)\n",
    "        y = self.layer_norm3(x)\n",
    "\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mwtQ02ReC7e"
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "\n",
    "dec_feats = torch.randn((3, 10, 16))\n",
    "dec_mask = torch.randn((3, 1, 10, 10)) > 0.5\n",
    "\n",
    "enc_feats = torch.randn((3, 12, 16))\n",
    "enc_mask = torch.randn((3, 1, 1, 12)) > 0.5\n",
    "\n",
    "model = TransformerDecoderCell(16, 2, 32, 0.1)\n",
    "z = model(dec_feats, enc_feats, enc_mask, dec_mask)\n",
    "assert len(z.shape) == len(dec_feats.shape)\n",
    "for dim_z, dim_x in zip(z.shape, dec_feats.shape):\n",
    "    assert dim_z == dim_x\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5YzXlgzd6v_"
   },
   "source": [
    "### 3.1.2 Building the Decoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xgx1x5_eeAGe"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A TransformerDecoder is a stack of multiple TransformerDecoderCells and a Layer Norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: How many TransformerDecoderCells in stack\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        # Store a stack of TranformerDecoderCells. \n",
    "        # Define a layer normalization layer to process the output of the entire encoder.                                           #\n",
    "\n",
    "        self.cells = nn.ModuleList([TransformerDecoderCell(input_dim, num_heads, ff_dim, dropout) for _ in range(num_cells)])\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
    "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
    "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
    "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
    "        \"\"\"\n",
    "\n",
    "        y = None\n",
    "        \n",
    "        # Feed x into the stack of TransformerDecoderCells and then normalize the output                          #\n",
    "\n",
    "        for cell in self.cells:\n",
    "            x = cell(x, encoder_output, src_mask, tgt_mask)\n",
    "        y = self.norm(x)\n",
    "\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reAYOrJfeHEj"
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "dec_feats = torch.randn((3, 10, 16))\n",
    "dec_mask = torch.randn((3, 1, 10, 10)) > 0.5\n",
    "\n",
    "enc_feats = torch.randn((3, 12, 16))\n",
    "enc_mask = torch.randn((3, 1, 1, 12)) > 0.5\n",
    "\n",
    "model = TransformerDecoder(16, 2, 32, 2, 0.1)\n",
    "z = model(dec_feats, enc_feats, enc_mask, dec_mask)\n",
    "assert len(z.shape) == len(dec_feats.shape)\n",
    "for dim_z, dim_x in zip(z.shape, dec_feats.shape):\n",
    "    assert dim_z == dim_x\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW7rCvoBeIPx"
   },
   "source": [
    "## 3.2 Transformer Based Seq-to-Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLiV0CqNeOFm"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based sequence-to-sequence model.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            num_encoder_layers: int, num_decoder_layers: int, embed_dim: int,\n",
    "            num_heads: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "            trx_ff_dim: int = 512, dropout: float = 0.1, pad_token: int=0\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - num_encoder_layers: How many TransformerEncoderCell in stack\n",
    "        - num_decoder_layers: How many TransformerDecoderCell in stack\n",
    "        - embed_dim: Word embeddings dimension\n",
    "        - num_heads: Number of attention heads\n",
    "        - src_vocab_size: Number of tokens in the source language vocabulary\n",
    "        - tgt_vocab_size: Number of tokens in the target language vocabulary\n",
    "        - trx_ff_dim: Hidden dimension in the feedforward network\n",
    "        - dropout: Dropout ratio\n",
    "        \"\"\"\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Word embeddings for both the source and target languages\n",
    "        self.src_token_embed = nn.Embedding(src_vocab_size, embed_dim, padding_idx=pad_token)\n",
    "        self.tgt_token_embed = nn.Embedding(tgt_vocab_size, embed_dim, padding_idx=pad_token)\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Define the positional encoding, encoder, decoder, and the output  #\n",
    "        # layer. Think of how many classes are in the output layer.               #\n",
    "        ###########################################################################\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(embed_dim, num_heads, trx_ff_dim, num_encoder_layers, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(embed_dim, num_heads, trx_ff_dim, num_decoder_layers, dropout)\n",
    "        self.output_layer = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - src: Tensor of BxLe, word indexes in the source language\n",
    "        - tgt: Tensor of BxLd, word indexes in the target language\n",
    "        - src_mask: Tensor, masks of the tokens on the encoder side\n",
    "        - tgt_mask: Tensor, masks of the tokens on the decoder side\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of BxLdxK. K is the number of classes in the output.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get word embeddings. Note they are scaled.\n",
    "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
    "        tgt_embed = self.tgt_token_embed(tgt) * math.sqrt(self.embed_dim)\n",
    "\n",
    "        logits = None\n",
    "        \n",
    "        # Add positional encodings to the word embeddings. \n",
    "        # Feed them then to the encoder and decoder.\n",
    "        # Get the logits finally. \n",
    "        \n",
    "        src_pe = self.positional_encoding(src_embed)\n",
    "        tgt_pe = self.positional_encoding(tgt_embed)\n",
    "        enc_out = self.transformer_encoder(src_pe, src_mask)\n",
    "        dec_out = self.transformer_decoder(tgt_pe, enc_out, src_mask, tgt_mask)\n",
    "        #print(\"shape1\")\n",
    "        #print(x.shape)\n",
    "        #print(\"shape2\")\n",
    "        #print(x.shape)\n",
    "        logits = self.output_layer(dec_out)\n",
    "\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def encode(self, src:  torch.Tensor, src_mask:  torch.Tensor):\n",
    "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
    "        return self.transformer_encoder(self.positional_encoding(src_embed), src_mask)\n",
    "\n",
    "    def decode(self, tgt:  torch.Tensor, memory:  torch.Tensor, src_mask:  torch.Tensor, tgt_mask:  torch.Tensor):\n",
    "        tgt_embed = self.tgt_token_embed(tgt) * math.sqrt(self.embed_dim)\n",
    "        return self.transformer_decoder(self.positional_encoding(tgt_embed), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V7Sr6AIeRzL"
   },
   "outputs": [],
   "source": [
    "src_vocab_size = 10\n",
    "src = torch.arange(src_vocab_size).view(1, -1)\n",
    "src = torch.cat((src, src), dim=0)\n",
    "src_mask = torch.randn((2, 1, 1, src_vocab_size)) > 0.5\n",
    "\n",
    "tgt_vocab_size = 12\n",
    "tgt = torch.arange(tgt_vocab_size).view(1, -1)\n",
    "tgt = torch.cat((tgt, tgt), dim=0)\n",
    "tgt_mask = torch.randn((2, 1, tgt_vocab_size, tgt_vocab_size)) > 0.5\n",
    "\n",
    "model = Seq2SeqTransformer(2, 2, 16, 2, src_vocab_size, tgt_vocab_size, 32, 0.1, 0)\n",
    "z = model(src, tgt, src_mask, tgt_mask)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqgiM4hden4B"
   },
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2IKSRZ1eUDw"
   },
   "source": [
    "Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUx8QDwieWZW"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "def create_mask(src, tgt, pad_token=0):\n",
    "    src_mask = (src != pad_token).unsqueeze(-2).unsqueeze(1)\n",
    "\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "    tgt_mask = (tgt != pad_token).unsqueeze(-2)\n",
    "    # submask --> 3d vector --> batch x tokens x tokens --> masks whats behind each row\n",
    "    # triu --> sets the lower diagonal of matrix so its 0\n",
    "    # Use a BERT pre-trained --> attention masks\n",
    "    tgt_mask = tgt_mask & subsequent_mask(tgt.shape[1]).type_as(tgt_mask.data)\n",
    "\n",
    "    return src_mask, tgt_mask.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1699287507771,
     "user": {
      "displayName": "Aanand .D",
      "userId": "04055414698771147992"
     },
     "user_tz": 300
    },
    "id": "iLielDIoeYAc",
    "outputId": "39dae3bd-832c-420e-c762-8cad8a4b05ae"
   },
   "outputs": [],
   "source": [
    "# Visualize what the target mask looks like\n",
    "\n",
    "sns.set_context(context=\"talk\")\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0].numpy())\n",
    "\n",
    "x = torch.arange(src_vocab_size).view(1, -1)\n",
    "x = torch.cat((x, x), dim=0)\n",
    "src_mask, tgt_mask = create_mask(x, x)\n",
    "print(src_mask.shape, tgt_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxTO4EJted8m"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running this on collab I need en and de\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcTVDIentHiu"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k\n",
    "\n",
    "# Update URLs to point to data stored by user\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
    "\n",
    "# Update hash since there is a discrepancy between user hosted test split and that of the test split in the original dataset\n",
    "multi30k.MD5[\"test\"] = \"6d1ca1dba99e2c5dd54cae1226ff11c2551e6ce63527ebb072a1f70f72a5cd36\"\n",
    "\n",
    "data_train = Multi30k(split='train')\n",
    "data_val = Multi30k(split='valid')\n",
    "data_test = Multi30k(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO56fT5JeeuX"
   },
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# # Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(\n",
    "        token_transform[ln], #Tokenization\n",
    "        vocab_transform[ln], #Numericalization\n",
    "        tensor_transform # Add BOS/EOS and create tensor\n",
    "    )\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch.transpose(0, 1), tgt_batch.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwZZas2Hk9zx"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "for idx, (src, tgt) in enumerate(train_dataloader):\n",
    "    if idx > 2:\n",
    "        break\n",
    "    print('src: {}, tgt: {}'.format(src.shape, tgt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3vT-8QKermB"
   },
   "source": [
    "## 3.3 Model HyperParameters & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fskKTE7Nevot"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMBED_SIZE = 512\n",
    "NUM_ATTN_HEADS = 8\n",
    "FF_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = None\n",
    "\n",
    "# Reminder:\n",
    "#          def __init__(self,\n",
    "#            num_encoder_layers: int, num_decoder_layers: int, embed_dim: int,\n",
    "#            num_heads: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "#            trx_ff_dim: int = 512, dropout: float = 0.1, pad_token: int=0\n",
    "\n",
    "\n",
    "\n",
    "# Define model and loss function. \n",
    "# IGNORE PADDING TOKENS!!\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMBED_SIZE,\n",
    "                                 NUM_ATTN_HEADS, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    "                                 FF_DIM, 0.1, PAD_IDX)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(),\n",
    "    lr=0.0001,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tA7ZPoke4bV"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
    "        src_mask = src_mask.to(device)\n",
    "        tgt_mask = tgt_mask.to(device)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Should be able to get train loss around 1.5 and val loss around 2.2\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "# Epoch: 20, Train loss: 0.096, Val loss: 0.577, Epoch time = 41.827s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r29zkV8YA3iC"
   },
   "source": [
    "## Greedy Translate/Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsiMgRWwjNbQ"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.float).to(device)\n",
    "    for i in range(max_len-1):\n",
    "\n",
    "        memory = memory.to(device)\n",
    "        tgt_mask = (subsequent_mask(ys.size(1))\n",
    "                    .type(torch.bool)).to(device)\n",
    "        cross_mask = torch.ones(1,i + 1,src.size(1), device=device)\n",
    "        out = model.decode(ys.long(), memory, cross_mask, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.output_layer(out[:, -1])\n",
    "        _, next_word = torch.max(prob[-1,:], dim=0)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(1, -1)\n",
    "    num_tokens = src.shape[1]\n",
    "    src_mask = (torch.zeros(1, num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens, model = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX)\n",
    "\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.flatten().cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Qii2NvBvQ4"
   },
   "outputs": [],
   "source": [
    "src_sentence = \"Eine Gruppe von Menschen steht vor einem Iglu .\"\n",
    "translate(transformer, src_sentence)\n",
    "# not great more training needed \" . the \""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
